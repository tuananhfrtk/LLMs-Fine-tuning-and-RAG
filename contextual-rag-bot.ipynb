{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q faiss-cpu rank_bm25 langchain-cohere ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T22:42:53.258294Z","iopub.execute_input":"2025-03-18T22:42:53.258578Z","iopub.status.idle":"2025-03-18T22:43:08.880803Z","shell.execute_reply.started":"2025-03-18T22:42:53.258556Z","shell.execute_reply":"2025-03-18T22:43:08.879961Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.7/30.7 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m72.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m415.9/415.9 kB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m73.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"!pip install langchain-openai","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T22:43:13.710230Z","iopub.execute_input":"2025-03-18T22:43:13.710532Z","iopub.status.idle":"2025-03-18T22:43:18.680696Z","shell.execute_reply.started":"2025-03-18T22:43:13.710506Z","shell.execute_reply":"2025-03-18T22:43:18.679808Z"}},"outputs":[{"name":"stdout","text":"Collecting langchain-openai\n  Downloading langchain_openai-0.3.9-py3-none-any.whl.metadata (2.3 kB)\nRequirement already satisfied: langchain-core<1.0.0,>=0.3.45 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.3.45)\nCollecting openai<2.0.0,>=1.66.3 (from langchain-openai)\n  Downloading openai-1.66.5-py3-none-any.whl.metadata (24 kB)\nRequirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.10/dist-packages (from langchain-openai) (0.9.0)\nRequirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (0.2.3)\nRequirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (9.0.0)\nRequirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (1.33)\nRequirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (6.0.2)\nRequirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (24.2)\nRequirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (4.12.2)\nRequirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<1.0.0,>=0.3.45->langchain-openai) (2.11.0a2)\nRequirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (3.7.1)\nRequirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (1.9.0)\nRequirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (0.28.1)\nRequirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (0.8.2)\nRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (1.3.1)\nRequirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.66.3->langchain-openai) (4.67.1)\nRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\nRequirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2.32.3)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.66.3->langchain-openai) (3.10)\nRequirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.66.3->langchain-openai) (1.2.2)\nRequirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai) (2025.1.31)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai) (1.0.7)\nRequirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai<2.0.0,>=1.66.3->langchain-openai) (0.14.0)\nRequirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.45->langchain-openai) (3.0.0)\nRequirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain-openai) (3.10.12)\nRequirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<1.0.0,>=0.3.45->langchain-openai) (1.0.0)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.45->langchain-openai) (0.7.0)\nRequirement already satisfied: pydantic-core==2.29.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<1.0.0,>=0.3.45->langchain-openai) (2.29.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (3.4.1)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken<1,>=0.7->langchain-openai) (2.3.0)\nDownloading langchain_openai-0.3.9-py3-none-any.whl (60 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.9/60.9 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading openai-1.66.5-py3-none-any.whl (571 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.1/571.1 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: openai, langchain-openai\n  Attempting uninstall: openai\n    Found existing installation: openai 1.57.4\n    Uninstalling openai-1.57.4:\n      Successfully uninstalled openai-1.57.4\nSuccessfully installed langchain-openai-0.3.9 openai-1.66.5\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# from kaggle_secrets import UserSecretsClient\n# user_secrets = UserSecretsClient()\n# hugging_face_token = user_secrets.get_secret(\" HUGGINGFACE_TOKEN\")\n# cohere_api_key = user_secrets.get_secret(\"COHERE_API_KEY\")\n# groq_api_key = user_secrets.get_secret(\"GROQ_API_KEY\")\n# wandb_api_key = user_secrets.get_secret(\"WANDB_API_KEY\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T21:17:40.541472Z","iopub.execute_input":"2025-03-18T21:17:40.541831Z","iopub.status.idle":"2025-03-18T21:17:41.696691Z","shell.execute_reply.started":"2025-03-18T21:17:40.541804Z","shell.execute_reply":"2025-03-18T21:17:41.695805Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Enable parallel tokenization\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n\n# Retrieve and set Cohere API Key\nos.environ[\"COHERE_API_KEY\"] = user_secrets.get_secret(\"COHERE_API_KEY\")\n\n# Retrieve and set Hugging face API Key\nos.environ[\" HUGGINGFACE_TOKEN\"] = user_secrets.get_secret(\" HUGGINGFACE_TOKEN\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T22:43:49.337406Z","iopub.execute_input":"2025-03-18T22:43:49.337767Z","iopub.status.idle":"2025-03-18T22:43:49.576373Z","shell.execute_reply.started":"2025-03-18T22:43:49.337732Z","shell.execute_reply":"2025-03-18T22:43:49.575445Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"import os\nfrom kaggle_secrets import UserSecretsClient\nfrom openai import OpenAI\n\n# Retrieve the API key from Kaggle secrets\nuser_secrets = UserSecretsClient()\ngroq_api_key = user_secrets.get_secret(\"GROQ_API_KEY\")\n\n# Set the environment variable\nos.environ[\"GROQ_API_KEY\"] = groq_api_key\n\n# Initialize OpenAI-compatible client for Groq\nclient = OpenAI(\n    api_key=os.getenv(\"GROQ_API_KEY\"),\n    base_url=\"https://api.groq.com/openai/v1\"\n)\n\n# Test request with Groq's Llama 3 model\nresponse = client.chat.completions.create(\n    model=\"llama3-8b-8192\",  # Other options: \"mixtral-8x7b-32768\", \"gemma-7b-it\"\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"What is Das Kapital?\"}\n    ]\n)\n\n# Print the response\nprint(response.choices[0].message.content)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T22:43:41.102890Z","iopub.execute_input":"2025-03-18T22:43:41.103218Z","iopub.status.idle":"2025-03-18T22:43:42.500348Z","shell.execute_reply.started":"2025-03-18T22:43:41.103194Z","shell.execute_reply":"2025-03-18T22:43:42.499641Z"}},"outputs":[{"name":"stdout","text":"Das Kapital, literally \"The Capital\" in German, is a foundational work of Marxism written by Karl Marx in 1867. It's a critique of capitalist society and an analysis of the economic systems that existed in the 19th century.\n\nDas Kapital focuses on the concept of surplus value, which Marx believed was the source of profit for capitalist enterprises. According to Marx, workers produce value but are paid less than the value they create, resulting in surplus value that is extracted by the capitalist class. He argued that this exploitation is the fundamental driving force behind the dynamic and constantly changing nature of capitalist societies.\n\nThe book is divided into three volumes, each exploring different aspects of capitalism:\n\n1. Das Kapital, Volume I: Marx analyzes the inner workings of capitalist production, discussing issues like the labor theory of value, alienation of labor, and the concept of surplus value.\n2. Das Kapital, Volume II: This volume examines the circulation of capital, discussing topics such as credit, finance, and international trade.\n3. Das Kapital, Volume III: The final volume is a more philosophical and philosophical critique of capitalism, discussing the general laws of capitalist development and the inevitability of its collapse.\n\nDas Kapital has had a profound impact on modern economics and political thought, influencing a wide range of thinkers, including Vladimir Lenin and Mao Zedong, among many others. While some critics have accused Marx of being overly critical and polemical, his work remains a landmark in the development of modern economic thought.\n\nWould you like to know more about Marx's ideas on commodity fetishism or the labor theory of value?\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"from langchain.document_loaders import TextLoader\nfrom langchain.prompts import PromptTemplate\nfrom langchain.retrievers import BM25Retriever\nfrom langchain.vectorstores import FAISS\nfrom langchain_cohere import CohereRerank\nfrom langchain_openai import AzureChatOpenAI, AzureOpenAIEmbeddings","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T22:43:53.017141Z","iopub.execute_input":"2025-03-18T22:43:53.017426Z","iopub.status.idle":"2025-03-18T22:43:54.442623Z","shell.execute_reply.started":"2025-03-18T22:43:53.017403Z","shell.execute_reply":"2025-03-18T22:43:54.441981Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import os\ndataset_path = \"/kaggle/input/das-kapital\"\nprint(\"Files inside dataset:\", os.listdir(dataset_path))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T22:43:56.844949Z","iopub.execute_input":"2025-03-18T22:43:56.845238Z","iopub.status.idle":"2025-03-18T22:43:56.862163Z","shell.execute_reply.started":"2025-03-18T22:43:56.845217Z","shell.execute_reply":"2025-03-18T22:43:56.860948Z"}},"outputs":[{"name":"stdout","text":"Files inside dataset: ['DasKapital (Volume III).txt', 'DasKapital (Volume II).txt', 'DasKapital (Volume I).txt']\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"from datasets import Dataset\n\n# Function to read and clean text\ndef read_and_clean(file_path):\n    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n        text = file.read()\n    return \" \".join(text.split())  # Normalize spaces\n\n# Read all text files\nfile_paths = [os.path.join(dataset_path, f) for f in os.listdir(dataset_path) if f.endswith(\".txt\")]\ntexts = [read_and_clean(f) for f in file_paths]\n\n# Create a Hugging Face Dataset\ndataset = Dataset.from_dict({\"text\": texts})\n\n# Show the first 500 characters\nprint(\"Dataset loaded successfully!\")\nprint(dataset[0][\"text\"][:500])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T22:43:59.583170Z","iopub.execute_input":"2025-03-18T22:43:59.583486Z","iopub.status.idle":"2025-03-18T22:43:59.904731Z","shell.execute_reply.started":"2025-03-18T22:43:59.583456Z","shell.execute_reply":"2025-03-18T22:43:59.903936Z"}},"outputs":[{"name":"stdout","text":"Dataset loaded successfully!\nCapital, Vol.3, Table of Contents Karl Marx's CAPITAL Vol. Ill THE PROCESS OF CAPITALIST PRODUCTION AS A WHOLE Written: Karl Marx, 1863-1883, edited by Friedrick Engels and completed by him eleven years after Marx's death. Source: Institute of Marxism-Leninism, USSR, 1959 Publisher: International Publishers, NY, [n.d.] First Published: 1894 Translated: On-Line Version: Marx.org 1996, Marxists.org 1999 Transcribed: Transcribed for the Internet in 1996 by Hinrich Kuhls and Zodiac, and by Tim Delan\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from langchain.chat_models import ChatOpenAI\nfrom langchain.embeddings import HuggingFaceEmbeddings\n\n# Initialize the LLM using Groq's OpenAI-compatible API\nllm = ChatOpenAI(\n    openai_api_key=os.getenv(\"GROQ_API_KEY\"),\n    openai_api_base=\"https://api.groq.com/openai/v1\",\n    model_name=\"mixtral-8x7b-32768\",  # Other options: \"mixtral-8x7b-32768\", \"gemma-7b-it\"\n    temperature=0.0,\n)\n\n# Initialize Hugging Face embeddings\nembeddings = HuggingFaceEmbeddings(\n    model_name=\"BAAI/bge-small-en\",  # Replace with your preferred model\n    model_kwargs={\"device\": \"cuda\"}  # Use GPU if available\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T23:09:42.758789Z","iopub.execute_input":"2025-03-18T23:09:42.759185Z","iopub.status.idle":"2025-03-18T23:09:43.637455Z","shell.execute_reply.started":"2025-03-18T23:09:42.759159Z","shell.execute_reply":"2025-03-18T23:09:43.636462Z"}},"outputs":[],"execution_count":36},{"cell_type":"markdown","source":"### LOAD DATA","metadata":{}},{"cell_type":"code","source":"WHOLE_DOCUMENT = dataset[0][\"text\"]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T23:09:46.883619Z","iopub.execute_input":"2025-03-18T23:09:46.883980Z","iopub.status.idle":"2025-03-18T23:09:46.890208Z","shell.execute_reply.started":"2025-03-18T23:09:46.883953Z","shell.execute_reply":"2025-03-18T23:09:46.889311Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"prompt_document = PromptTemplate(\n    input_variables=[\"WHOLE_DOCUMENT\"], template=\"{WHOLE_DOCUMENT}\"\n)\nprompt_chunk = PromptTemplate(\n    input_variables=[\"CHUNK_CONTENT\"],\n    template=\"Here is the chunk we want to situate within the whole document\\n\\n{CHUNK_CONTENT}\\n\\n\"\n    \"Please give a short succinct context to situate this chunk within the overall document for \"\n    \"the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.\",\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T23:09:53.412022Z","iopub.execute_input":"2025-03-18T23:09:53.412306Z","iopub.status.idle":"2025-03-18T23:09:53.416191Z","shell.execute_reply.started":"2025-03-18T23:09:53.412287Z","shell.execute_reply":"2025-03-18T23:09:53.415447Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"from langchain.text_splitter import RecursiveCharacterTextSplitter\nfrom langchain_core.documents import BaseDocumentCompressor\nfrom langchain_core.retrievers import BaseRetriever\n\n\ndef split_text(texts):\n    text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=200)\n    doc_chunks = text_splitter.create_documents(texts)\n    for i, doc in enumerate(doc_chunks):\n        # Append a new Document object with the appropriate doc_id\n        doc.metadata = {\"doc_id\": f\"doc_{i}\"}\n    return doc_chunks\n\n\ndef create_embedding_retriever(documents_):\n    vector_store = FAISS.from_documents(documents_, embedding=embeddings)\n    return vector_store.as_retriever(search_kwargs={\"k\": 4})\n\n\ndef create_bm25_retriever(documents_):\n    retriever = BM25Retriever.from_documents(documents_, language=\"english\")\n    return retriever\n\n\n# Function to create a combined embedding and BM25 retriever with reranker\nclass EmbeddingBM25RerankerRetriever:\n    def __init__(\n        self,\n        vector_retriever: BaseRetriever,\n        bm25_retriever: BaseRetriever,\n        reranker: BaseDocumentCompressor,\n    ):\n        self.vector_retriever = vector_retriever\n        self.bm25_retriever = bm25_retriever\n        self.reranker = reranker\n\n    def invoke(self, query: str):\n        vector_docs = self.vector_retriever.invoke(query)\n        bm25_docs = self.bm25_retriever.invoke(query)\n\n        combined_docs = vector_docs + [\n            doc for doc in bm25_docs if doc not in vector_docs\n        ]\n\n        reranked_docs = self.reranker.compress_documents(combined_docs, query)\n        return reranked_docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T23:09:56.118916Z","iopub.execute_input":"2025-03-18T23:09:56.119235Z","iopub.status.idle":"2025-03-18T23:09:56.126569Z","shell.execute_reply.started":"2025-03-18T23:09:56.119207Z","shell.execute_reply":"2025-03-18T23:09:56.125749Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"chunks = split_text([WHOLE_DOCUMENT])\n\nembedding_retriever = create_embedding_retriever(chunks)\n\n# Define a BM25 retriever\nbm25_retriever = create_bm25_retriever(chunks)\n\nreranker = CohereRerank(top_n=3, model=\"rerank-english-v2.0\")\n\n# Create combined retriever\nembedding_bm25_retriever_rerank = EmbeddingBM25RerankerRetriever(\n    vector_retriever=embedding_retriever,\n    bm25_retriever=bm25_retriever,\n    reranker=reranker,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T23:09:59.024497Z","iopub.execute_input":"2025-03-18T23:09:59.024881Z","iopub.status.idle":"2025-03-18T23:10:06.803719Z","shell.execute_reply.started":"2025-03-18T23:09:59.024847Z","shell.execute_reply":"2025-03-18T23:10:06.803030Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"# import tqdm as tqdm\n# from langchain.docstore.document import Document\n\n\n# def create_contextual_chunks(chunks_):\n#     # uses a llm to add context to each chunk given the prompts defined above\n#     contextual_documents = []\n#     for chunk in tqdm.tqdm(chunks_):\n#         context = prompt_document.format(WHOLE_DOCUMENT=WHOLE_DOCUMENT)\n#         chunk_context = prompt_chunk.format(CHUNK_CONTENT=chunk)\n#         llm_response = llm.invoke(context + chunk_context).content\n#         page_content = f\"\"\"Text: {chunk.page_content}\\n\\n\\nContext: {llm_response}\"\"\"\n#         doc = Document(page_content=page_content, metadata=chunk.metadata)\n#         contextual_documents.append(doc)\n#     return contextual_documents\n# contextual_documents = create_contextual_chunks(chunks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T23:10:17.309735Z","iopub.execute_input":"2025-03-18T23:10:17.310032Z","iopub.status.idle":"2025-03-18T23:10:17.314763Z","shell.execute_reply.started":"2025-03-18T23:10:17.310009Z","shell.execute_reply":"2025-03-18T23:10:17.314056Z"}},"outputs":[],"execution_count":41},{"cell_type":"code","source":"import tqdm\nfrom langchain.docstore.document import Document\n\n# Define a max token limit per request (Keep well below Groq's daily limit)\nMAX_TOKENS_PER_CHUNK = 4000  # Adjust as needed\n\n# Function to split long chunks before sending them to the LLM\ndef split_chunk(chunk, max_tokens=MAX_TOKENS_PER_CHUNK):\n    words = chunk.page_content.split()\n    sub_chunks = [\" \".join(words[i:i + max_tokens]) for i in range(0, len(words), max_tokens)]\n    return sub_chunks\n\n# Function to create contextual chunks with optimized size\ndef create_contextual_chunks(chunks_):\n    contextual_documents = []\n    \n    for chunk in tqdm.tqdm(chunks_):\n        sub_chunks = split_chunk(chunk)  # Reduce chunk size before sending\n        for sub_chunk in sub_chunks:\n            context = prompt_document.format(WHOLE_DOCUMENT=WHOLE_DOCUMENT[:1000])  # Truncate document context\n            chunk_context = prompt_chunk.format(CHUNK_CONTENT=sub_chunk)  \n            \n            # Send smaller request to LLM\n            llm_response = llm.invoke(context + chunk_context).content  \n            page_content = f\"\"\"Text: {sub_chunk}\\n\\n\\nContext: {llm_response}\"\"\"\n            \n            # Store results\n            doc = Document(page_content=page_content, metadata=chunk.metadata)\n            contextual_documents.append(doc)\n\n    return contextual_documents\n\n# Call the optimized function\ncontextual_documents = create_contextual_chunks(chunks)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-18T23:14:35.645021Z","iopub.execute_input":"2025-03-18T23:14:35.645359Z","iopub.status.idle":"2025-03-19T01:03:30.763744Z","shell.execute_reply.started":"2025-03-18T23:14:35.645336Z","shell.execute_reply":"2025-03-19T01:03:30.762154Z"}},"outputs":[{"name":"stderr","text":" 76%|███████▋  | 470/615 [1:48:55<33:36, 13.90s/it]  \n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-ba89c4eb7432>\u001b[0m in \u001b[0;36m<cell line: 34>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Call the optimized function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mcontextual_documents\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_contextual_chunks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-43-ba89c4eb7432>\u001b[0m in \u001b[0;36mcreate_contextual_chunks\u001b[0;34m(chunks_)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# Send smaller request to LLM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0mllm_response\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mchunk_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m             \u001b[0mpage_content\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\"\"Text: {sub_chunk}\\n\\n\\nContext: {llm_response}\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    284\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 286\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mStringPromptValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    287\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mChatPromptValue\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    784\u001b[0m                     \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_managers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mrun_managers\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    785\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 786\u001b[0;31m                 )\n\u001b[0m\u001b[1;32m    787\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m             ],\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mprovider\u001b[0m \u001b[0mAPI\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 643\u001b[0;31m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    644\u001b[0m             \u001b[0mAn\u001b[0m \u001b[0mLLMResult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwhich\u001b[0m \u001b[0mcontains\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mcandidate\u001b[0m \u001b[0mGenerations\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0meach\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    645\u001b[0m                 \u001b[0mprompt\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0madditional\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mprovider\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mspecific\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;36m3.\u001b[0m \u001b[0mare\u001b[0m \u001b[0mbuilding\u001b[0m \u001b[0mchains\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mare\u001b[0m \u001b[0magnostic\u001b[0m \u001b[0mto\u001b[0m \u001b[0mthe\u001b[0m \u001b[0munderlying\u001b[0m \u001b[0mlanguage\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    632\u001b[0m                 \u001b[0mtype\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpure\u001b[0m \u001b[0mtext\u001b[0m \u001b[0mcompletion\u001b[0m \u001b[0mmodels\u001b[0m \u001b[0mvs\u001b[0m \u001b[0mchat\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 633\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    634\u001b[0m         \u001b[0mArgs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    635\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mList\u001b[0m \u001b[0mof\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mCallbacks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m     ) -> LLMResult:\n\u001b[0m\u001b[1;32m    852\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m         return await self.agenerate(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/chat_models/openai.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    474\u001b[0m             \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    475\u001b[0m         }\n\u001b[0;32m--> 476\u001b[0;31m         response = self.completion_with_retry(\n\u001b[0m\u001b[1;32m    477\u001b[0m             \u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    478\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_community/chat_models/openai.py\u001b[0m in \u001b[0;36mcompletion_with_retry\u001b[0;34m(self, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    385\u001b[0m         \u001b[0;34m\"\"\"Use tenacity to retry the completion call.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_openai_v1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 387\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0mretry_decorator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_create_retry_decorator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/resources/chat/completions/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, web_search_options, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    912\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    913\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1240\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m         )\n\u001b[0;32m-> 1242\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1243\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1244\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    918\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 919\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    920\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    921\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1021\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1022\u001b[0m             \u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Re-raising status error\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1023\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_status_error_from_response\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1024\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m         return self._process_response(\n","\u001b[0;31mRateLimitError\u001b[0m: Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01jmrvjggffw8vf3vw6zy16n9e` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499143, Requested 1379. Please try again in 1m30.1506s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}"],"ename":"RateLimitError","evalue":"Error code: 429 - {'error': {'message': 'Rate limit reached for model `mixtral-8x7b-32768` in organization `org_01jmrvjggffw8vf3vw6zy16n9e` service tier `on_demand` on tokens per day (TPD): Limit 500000, Used 499143, Requested 1379. Please try again in 1m30.1506s. Need more tokens? Upgrade to Dev Tier today at https://console.groq.com/settings/billing', 'type': 'tokens', 'code': 'rate_limit_exceeded'}}","output_type":"error"}],"execution_count":43}]}